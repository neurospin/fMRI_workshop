{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
=======
>>>>>>> 67316f4 (updated basic decoding notebook)
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Different classifiers in decoding the Haxby dataset\n",
    "\n",
    "Here we compare different classifiers on a visual object recognition\n",
    "decoding task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by loading data using nilearn dataset fetcher\n",
    "from nilearn import datasets\n",
<<<<<<< HEAD
=======
    "from nilearn.image import get_data\n",
>>>>>>> 67316f4 (updated basic decoding notebook)
    "# by default 2nd subject data will be fetched\n",
    "haxby_dataset = datasets.fetch_haxby()\n",
    "\n",
    "# print basic information on the dataset\n",
    "print('First subject anatomical nifti image (3D) located is at: %s' %\n",
    "      haxby_dataset.anat[0])\n",
    "print('First subject functional nifti image (4D) is located at: %s' %\n",
<<<<<<< HEAD
    "      haxby_dataset.func[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load labels\n",
=======
    "      haxby_dataset.func[0])\n",
    "\n",
    "# load labels\n",
    "import numpy as np\n",
>>>>>>> 67316f4 (updated basic decoding notebook)
    "import pandas as pd\n",
    "labels = pd.read_csv(haxby_dataset.session_target[0], sep=\" \")\n",
    "stimuli = labels['labels']\n",
    "\n",
    "# identify resting state (baseline) labels in order to be able to remove them\n",
    "resting_state = (stimuli == 'rest')\n",
    "\n",
    "# extract the indices of the images corresponding to some condition or task\n",
<<<<<<< HEAD
    "import numpy as np\n",
=======
>>>>>>> 67316f4 (updated basic decoding notebook)
    "task_mask = np.logical_not(resting_state)\n",
    "\n",
    "# find names of remaining active labels\n",
    "categories = stimuli[task_mask].unique()\n",
<<<<<<< HEAD
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract tags indicating to which acquisition run a tag belongs\n",
    "session_labels = labels['chunks'][task_mask]\n",
    "\n",
    "# Load the fMRI data\n",
    "func_filename = haxby_dataset.func[0]\n",
    "\n",
    "# Load a more reduced mask of the ventral temporal regions provided as part of Haxby's dataset\n",
    "from nilearn import plotting\n",
    "mask_filename = haxby_dataset.mask_vt[0]\n",
    "plotting.plot_roi(mask_filename, bg_img=haxby_dataset.anat[0], dim=-1, cmap='Paired')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
=======
    "\n",
    "# extract tags indicating to which acquisition run a tag belongs\n",
    "session_labels = labels['chunks'][task_mask]\n",
    "\n",
    "\n",
    "# Load the fMRI data\n",
    "# For decoding, standardizing is often very important\n",
    "mask_filename = haxby_dataset.mask_vt[0]\n",
    "func_filename = haxby_dataset.func[0]\n",
    "\n",
    "\n",
>>>>>>> 67316f4 (updated basic decoding notebook)
    "# Because the data is in one single large 4D image, we need to use\n",
    "# index_img to do the split easily.\n",
    "from nilearn.image import index_img\n",
    "fmri_niimgs = index_img(func_filename, task_mask)\n",
<<<<<<< HEAD
    "classification_target = stimuli[task_mask]\n",
    "\n",
    "event_dictionary = {'onset': classification_target.index,\n",
    "                    'trial_type':classification_target.values,\n",
    "                    'duration': [1]* len(classification_target)}\n",
    "events = pd.DataFrame(event_dictionary )\n",
    "\n",
    "# plot the event structure\n",
    "from nilearn.plotting import plot_event, show\n",
    "plot_event(events, figsize=(15, 4))\n",
    "show()"
=======
    "classification_target = stimuli[task_mask]"
>>>>>>> 67316f4 (updated basic decoding notebook)
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the decoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we define the various classifiers that we use\n",
    "classifiers = ['svc_l2', 'svc_l1', 'logistic_l1',\n",
<<<<<<< HEAD
    "               'logistic_l2', 'ridge_classifier']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 'svc_l2', 'svc_l1' are two different flavors of SVC classifiers; the 'l2' one is non sparse and has a smooth behovior, the 'l1' leads to a sparse solution\n",
    "* 'logistic_l2' and 'logistic_l1' ar close cousins to the svc classifiers, with analog distinction between sparse and non-sparse solutions. Hyperparameters are a bit harder to tune with logitic classifiers.\n",
    "* 'ridge_classifier' is a simple regression model that tries to predict the binary outcome as if it were a continuous target\n",
    "\n",
    "Note that the use of these classifiers requires tuning some hyperparameters: these steps are hidden in the calssifier object. If you want to tune hyperparameters by yourself, make sure that you do this properly, using nested cross-validation https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
=======
    "               'logistic_l2', 'ridge_classifier']\n",
    "\n",
>>>>>>> 67316f4 (updated basic decoding notebook)
    "# Here we compute prediction scores and run time for all these\n",
    "# classifiers\n",
    "import time\n",
    "from nilearn.decoding import Decoder\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "cv = LeaveOneGroupOut()\n",
    "classifiers_data = {}\n",
    "\n",
    "for classifier_name in sorted(classifiers):\n",
    "    print(70 * '_')\n",
    "\n",
    "    # The decoder has as default score the `roc_auc`\n",
    "    decoder = Decoder(estimator=classifier_name, mask=mask_filename,\n",
    "                      standardize=True, cv=cv)\n",
    "    t0 = time.time()\n",
    "    decoder.fit(fmri_niimgs, classification_target, groups=session_labels)\n",
    "\n",
    "    classifiers_data[classifier_name] = {}\n",
    "    classifiers_data[classifier_name]['score'] = decoder.cv_scores_\n",
    "\n",
    "    print(\"%10s: %.2fs\" % (classifier_name, time.time() - t0))\n",
    "    for category in categories:\n",
    "        print(\"    %14s vs all -- AUC: %1.2f +- %1.2f\" % (\n",
    "            category,\n",
    "            np.mean(classifiers_data[classifier_name]['score'][category]),\n",
    "            np.std(classifiers_data[classifier_name]['score'][category]))\n",
    "        )\n",
    "\n",
    "    # Adding the average performance per estimator\n",
    "    scores = classifiers_data[classifier_name]['score']\n",
    "    scores['AVERAGE'] = np.mean(list(scores.values()), axis=0)\n",
    "    classifiers_data[classifier_name]['score'] = scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## Visualization of the results"
=======
    "## Visualization\n",
    "\n"
>>>>>>> 67316f4 (updated basic decoding notebook)
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we make a rudimentary diagram\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "all_categories = np.sort(np.hstack([categories, 'AVERAGE']))\n",
    "tick_position = np.arange(len(all_categories))\n",
    "plt.yticks(tick_position + 0.25, all_categories)\n",
    "height = 0.1\n",
    "\n",
    "for i, (color, classifier_name) in enumerate(zip(['b', 'm', 'k', 'r', 'g'],\n",
    "                                                 classifiers)):\n",
    "    score_means = [\n",
    "        np.mean(classifiers_data[classifier_name]['score'][category])\n",
    "        for category in all_categories\n",
    "    ]\n",
    "\n",
    "    plt.barh(tick_position, score_means,\n",
    "             label=classifier_name.replace('_', ' '),\n",
    "             height=height, color=color)\n",
    "    tick_position = tick_position + height\n",
    "\n",
    "plt.xlabel('Classification accuracy (AUC score)')\n",
    "plt.ylabel('Visual stimuli category')\n",
    "plt.xlim(xmin=0.5)\n",
    "plt.legend(loc='lower left', ncol=1)\n",
    "plt.title(\n",
    "    'Category-specific classification accuracy for different classifiers')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for a fixed penalty the results are similar between the svc\n",
    "and the logistic regression. The main difference relies on the penalty\n",
    "($\\ell_1$ and $\\ell_2$). The sparse penalty works better because we are in\n",
    "an intra-subject setting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## Multi-Class classification\n",
    "\n",
    "To deal with more than two classes, several strategies are possible https://scikit-learn.org/stable/modules/multiclass.html. The most frequent ones are one-versus-one and one-versus-all. We recomment using the latter.\n",
    "To get tighter control on the model, we will extract data arrays and directly use scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.maskers import NiftiMasker\n",
    "# For decoding, standardizing is often very important. Hence, standardize=True\n",
    "nifti_masker = NiftiMasker(mask_img=haxby_dataset.mask,\n",
    "                           standardize=True,\n",
    "                           runs=session_labels, \n",
    "                           smoothing_fwhm=4,\n",
    "                           memory=\"nilearn_cache\",\n",
    "                           memory_level=1)\n",
    "X = nifti_masker.fit_transform(fmri_niimgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build classifiers with 3 main ingredients\n",
    "# * An Anova-based feature selection strategy\n",
    "# * An SVC classifier, with linear kernel\n",
    "# * all this embedded in a given multi-class strategy: one-vs-one or one-vs-all\n",
    "# * the Anova + SVC are organized in a Pipeline object\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "svc_ovo = OneVsOneClassifier(Pipeline([\n",
    "    ('anova', SelectKBest(f_classif, k=500)),\n",
    "    ('svc', SVC(kernel='linear'))\n",
    "]))\n",
    "\n",
    "svc_ova = OneVsRestClassifier(Pipeline([\n",
    "    ('anova', SelectKBest(f_classif, k=500)),\n",
    "    ('svc', SVC(kernel='linear'))\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us compute cross-validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_scores_ovo = cross_val_score(\n",
    "    svc_ovo, X, classification_target, cv=cv, groups=session_labels, verbose=1)\n",
    "\n",
    "cv_scores_ova = cross_val_score(\n",
    "    svc_ova, X, classification_target, cv=cv, groups=session_labels, verbose=1)\n",
    "\n",
    "print('OvO:', cv_scores_ovo.mean())\n",
    "print('OvA:', cv_scores_ova.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the result\n",
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.boxplot([cv_scores_ova, cv_scores_ovo])\n",
    "plt.xticks([1, 2], ['One vs All', 'One vs One'])\n",
    "plt.title('Prediction: accuracy score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
=======
>>>>>>> 67316f4 (updated basic decoding notebook)
    "## Visualizing the face vs house map\n",
    "\n",
    "Restrict the decoding to face vs house\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_mask = np.logical_or(stimuli == 'face', stimuli == 'house')\n",
    "stimuli = stimuli[condition_mask]\n",
    "assert len(stimuli) == 216\n",
    "fmri_niimgs_condition = index_img(func_filename, condition_mask)\n",
    "session_labels = labels['chunks'][condition_mask]\n",
    "categories = stimuli.unique()\n",
    "assert len(categories) == 2\n",
    "\n",
    "for classifier_name in sorted(classifiers):\n",
    "    decoder = Decoder(estimator=classifier_name, mask=mask_filename,\n",
    "                      standardize=True, cv=cv)\n",
    "    decoder.fit(fmri_niimgs_condition, stimuli, groups=session_labels)\n",
    "    classifiers_data[classifier_name] = {}\n",
    "    classifiers_data[classifier_name]['score'] = decoder.cv_scores_\n",
    "    classifiers_data[classifier_name]['map'] = decoder.coef_img_['face']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we plot the face vs house map for the different classifiers\n",
    "Use the average EPI as a background\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.image import mean_img\n",
<<<<<<< HEAD
    "from nilearn.image import get_data\n",
=======
>>>>>>> 67316f4 (updated basic decoding notebook)
    "from nilearn.plotting import plot_stat_map, show\n",
    "mean_epi_img = mean_img(func_filename)\n",
    "\n",
    "for classifier_name in sorted(classifiers):\n",
    "    coef_img = classifiers_data[classifier_name]['map']\n",
    "    threshold = np.max(np.abs(get_data(coef_img))) * 1e-3\n",
    "    plot_stat_map(\n",
    "        coef_img, bg_img=mean_epi_img, display_mode='z', cut_coords=[-15],\n",
    "        threshold=threshold,\n",
    "        title='%s: face vs house' % classifier_name.replace('_', ' '))\n",
    "\n",
    "show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
