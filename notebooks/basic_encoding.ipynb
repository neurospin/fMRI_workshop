{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  NeuroSpin tutorial, subject-level encoding models (beginner level)\n",
    "__Content creator:__ Florent Meyniel, NeuroSpin, CEA Paris-Saclay\n",
    "\n",
    "I would like to acknowledge my colleagues [Le Ster _et al._ (2019)](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0225286) who shared their data on the OpenNeuro platform ([here](https://openneuro.org/datasets/ds002606)). This notebook analyzes a sample participant from this dataset.\n",
    "\n",
    "Note that for the purpose of speeding up computations in this tutorial, I have resampled the data to a coarse spatial resolution (4 mm isotropic), but the original data where collected at a beautiful 1.6 mm isotropic resolution on a 7T scanner.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up the environment\n",
    "\n",
    "The following cell installs the libraries that we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install libraries\n",
    "! pip install -U nilearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell downloads the data that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clone the git repository to get the data\n",
    "! git clone https://github.com/florentmeyniel/cogmaster_neuro102.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait until the download is complete. When you see the folder:\n",
    ">cogmaster_neuro102\n",
    "\n",
    "in the directory (in the left panel), you can continue and execute the next cell to move in the folder containing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd cogmaster_neuro102/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, do various imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm \n",
    "\n",
    "from nilearn.glm.first_level import make_first_level_design_matrix\n",
    "from nilearn.plotting import plot_design_matrix\n",
    "from nilearn.glm.first_level import run_glm\n",
    "from nilearn.glm import compute_contrast\n",
    "from nilearn.glm import fdr_threshold\n",
    "from nilearn import plotting\n",
    "from nilearn.maskers import NiftiMasker\n",
    "from nilearn import surface\n",
    "from nilearn import datasets\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Regression analysis and the General Linear Model (GLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example data set: A simple functional \"localizer\"\n",
    "\n",
    "The data we are going to analyze were collected during a \"localizer\" paradigm, developped by [P. Pinel et al, BMC 2007](https://bmcneurosci.biomedcentral.com/articles/10.1186/1471-2202-8-91).\n",
    "\n",
    "\n",
    "Here is (a portion of) the abstract of this paper:\n",
    ">Although cognitive processes such as reading and calculation are associated with reproducible cerebral networks, inter-individual variability is considerable. Understanding the origins of this variability will require the elaboration of large multimodal databases compiling behavioral, anatomical, genetic and functional neuroimaging data over hundreds of subjects. With this goal in mind, we designed a simple and fast acquisition procedure based on a 5-minute functional magnetic resonance imaging (fMRI) sequence that can be run as easily and as systematically as an anatomical scan, and is therefore used in every subject undergoing fMRI in our laboratory. This protocol captures the cerebral bases of auditory and visual perception, motor actions, reading, language comprehension and mental calculation at an individual level.\n",
    "\n",
    "Here is a figure of the paper that describes the task:\n",
    "![pinel](https://media.springernature.com/full/springer-static/image/art%3A10.1186%2F1471-2202-8-91/MediaObjects/12868_2007_Article_389_Fig1_HTML.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a design matrix\n",
    "We will build a design matrix that corresponds to this task. The following file lists the events and their timing in the task.\n",
    "\n",
    "Execute the code that loads the data and browse its content.  \n",
    "**Q1-1: Given the label of trials, is this design amenable to categorical or parametric regressors?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load event file\n",
    "task_file = 'sub-01_ses-01_locAP-sms-1-6iso-events.tsv'\n",
    "events = pd.read_table(task_file)\n",
    "events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will construct a design matrix for this task.  \n",
    "Note that the ten first columns correspond to task-related regressors.  \n",
    "**Q1-2: Explain how those regressors are constructed**  \n",
    "\n",
    "The design matrix also has columns mvt0 to mvt5 corresponding to the volume-to-volume movements of subjects (3 parameters for translation along x, y, z axis, and 3 parameters for rotation pitch, roll, yaw), and drift parameters (drift_1 to drift_4).  \n",
    "**Q1-3: Why do we include those extra regressors in the design matrix?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load movement parameters\n",
    "movement = pd.read_csv('rp_asub-01_locAP_multiband_1_6iso.txt', sep='\\s+',\n",
    "                       header=None, names=[f\"mvt{k}\" for k in range(6)])\n",
    "\n",
    "# get frame times (when each frame, a.k.a. volume, is collected)\n",
    "TR = 1.2285\n",
    "frame_times = np.arange(movement.shape[0])*TR\n",
    "\n",
    "# make design matrix\n",
    "design_matrix = make_first_level_design_matrix(frame_times,\n",
    "                                               events, \n",
    "                                               drift_model='cosine',\n",
    "                                               high_pass=1/128,\n",
    "                                               add_regs=movement,\n",
    "                                               add_reg_names=[name for name in movement.columns])\n",
    "plot_design_matrix(design_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell of code will select one column: the \"audio_left_hand\" column. The blue curve is the predicted fMRI timecourse elicited by this stimulus, and the dots denote the exact timing of those events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(frame_times, design_matrix['audio_left_hand'], label='predicted BOLD')\n",
    "plt.plot(events.loc[events['trial_type']=='audio_left_hand', 'onset'].values, \n",
    "         [0]*len(events.loc[events['trial_type']=='audio_left_hand', 'onset'].values),\n",
    "         'o', label='left click, audio instruction')\n",
    "plt.legend()\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Signal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2-4: We often say that the fMRI response is indirect, delayed and slow. Why?**  \n",
    "\n",
    "**Q2-5: The last peak is nearly twice as large as the other peaks. Why?**  \n",
    "Hint: you may want to zoom onto what happened around 250s in the task with the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events[(events[\"onset\"]>250) & (events[\"onset\"]<265)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of one voxel\n",
    "We are now going to analyze the activity of a single example voxel. First, load the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "masker = NiftiMasker(mask_strategy='epi',\n",
    "                    detrend=True,\n",
    "                    high_pass=1/128,\n",
    "                    t_r=TR)\n",
    "fMRI_data = masker.fit_transform('swtrasub-01_locAP_multiband_4mm.nii')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now extract an example voxel and plot its activity.  \n",
    "**Q1-6: Describe the timeseries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's focus on one voxel\n",
    "voxel_index = 4999\n",
    "example_voxel = fMRI_data[:, voxel_index]\n",
    "\n",
    "# and plot it's activity\n",
    "plt.plot(frame_times, fMRI_data[:, voxel_index])\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('BOLD signal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will compute and plot the predicted fMRI signal triggered by three types of events:\n",
    "- a click with the left hand\n",
    "- a click with the right hand\n",
    "- flashing a checkerboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot prediction signal\n",
    "predicted = {'left_click': design_matrix['audio_left_hand'].values + design_matrix['video_left_hand'].values,\n",
    "             'right_click':design_matrix['audio_right_hand'].values + design_matrix['video_right_hand'].values,\n",
    "              'checkerboard': design_matrix['horizontal_checkerboard'].values + \\\n",
    "             design_matrix['vertical_checkerboard'].values}\n",
    "\n",
    "for k, stimulus in enumerate(predicted):\n",
    "    plt.subplot(3,1,k+1)\n",
    "    plt.plot(frame_times, predicted[stimulus])\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('BOLD signal')\n",
    "    plt.title(stimulus)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2-7: Compare visually the observed timeseries and those three predicted timeseries. Which one is most similar to the observed timeseries?**  \n",
    "\n",
    "The following cell plots the observed and predicted signal one against the other.  \n",
    "**Q2-8: Interpret the X-Y plot below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predicted vs. observed\n",
    "for k, stimulus in enumerate(predicted):\n",
    "    plt.subplot(1,3,k+1)\n",
    "    plt.plot(predicted[stimulus], example_voxel, '.')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Observed')\n",
    "    plt.title(stimulus)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1-9 What is a regression weight (a.k.a. \"beta\")?**  \n",
    "The cell below computes and plots the (ordinary least square) estimate of the regression weights.  \n",
    "Note that I use the formula below. In practice, this is packaged in your analysis software (as we will see later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize regressors\n",
    "X = design_matrix.values\n",
    "Xz = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "Xz[:,-1] = 1\n",
    "\n",
    "# Compute the regression weights\n",
    "beta = np.linalg.inv(Xz.T @ Xz) @ Xz.T @ example_voxel\n",
    "plt.bar(np.arange(len(beta)), beta)\n",
    "plt.xticks(ticks=np.arange(len(beta)),\n",
    "          labels=[name for name in design_matrix.columns],\n",
    "          rotation=60, ha='right')\n",
    "plt.ylabel('beta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regressors have been normalized so that the magnitude of the betas can be compared among each other.  \n",
    "**Q1-10: Interpret the betas. What appears to be a likely cause of the activity in this voxel?**  \n",
    "\n",
    "In the course, we saw that we can test effects in the data with contrasts. Below I specify two example contrasts:\n",
    "- \"left click elicits more signal than right click\" (displayed)\n",
    "- \"audio stimuli elicits more signal than visual stimuli\" (not displayed below).  \n",
    "\n",
    "**Q1-11: How is a contrast constructed?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify contrast\n",
    "contrasts = {\n",
    "    'left_hand': np.array([1 if 'left_hand' in name else 0 for name in design_matrix.columns]),\n",
    "    'right_hand': np.array([1 if 'right_hand' in name else 0 for name in design_matrix.columns]),\n",
    "    'audio': np.array([1 if 'audio' in name else 0 for name in design_matrix.columns]),\n",
    "    'video': np.array([1 if 'video' in name else 0 for name in design_matrix.columns]),}\n",
    "contrasts['left - right click'] = contrasts['left_hand'] - contrasts['right_hand']\n",
    "contrasts['audio - video'] = contrasts['audio'] - contrasts['video']\n",
    "\n",
    "# Show contrast\n",
    "print(contrasts['left - right click'])\n",
    "plt.bar(np.arange(len(contrasts['left - right click'])),\n",
    "        contrasts['left - right click'])\n",
    "plt.xticks(ticks=np.arange(len(contrasts['left - right click'])),\n",
    "          labels=[name for name in design_matrix.columns],\n",
    "          rotation=60, ha='right')\n",
    "plt.title('Contrast: '+ 'left - right click')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the name of the columns in the design matrix, **Q1-12: What does the following contrast correspond to?**\n",
    "[1 0 0 -1 0 0 1 0 0 -1 0 0 0 0 0 0 0 0 0 0 0]  \n",
    "\n",
    "Below I compute the t-value corresponding to the two contrasts \"left - right click\" and \"audio - video\".  \n",
    "**Q1-13: Which t-value is the largest? Interpret**  \n",
    "\n",
    "Note that I use the formula for educational purpose. This computation is also packaged in standard analysis softwares. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the t-value of two contrast\n",
    "def compute_tvalue(y, X, beta, c):\n",
    "    c = c[:, np.newaxis]\n",
    "    res = y - X @ beta\n",
    "    tval = c.T @ beta / (np.std(res) * np.sqrt(c.T @ np.linalg.inv(X.T @ X) @ c))\n",
    "    return tval[0][0]\n",
    "\n",
    "for contrast_id in ['left - right click', 'audio - video']:\n",
    "    print(contrast_id, ', tvalue=',\n",
    "          compute_tvalue(example_voxel, Xz, beta,\n",
    "                         contrasts[contrast_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If time permits, you can rerun the cells above, starting from \"# let's focus on one voxel\" where you change the index of the voxel (the original value is 4999, you can pick any value from 0 to 26942, which is the number of voxels in this data set). Execute the cells in the order in which they appear.\n",
    "\n",
    "In pratice, one does not compute the beta weights and tvalues by hand like I did here, but instead uses existing packages.\n",
    "For instance here is a [regression function](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) available in scikit-learn that returns the beta estimates.\n",
    "And another regression function that returns the beta estimates and their statistics (t-value, p-value, etc.) can be found in [statsmodel](https://www.statsmodels.org/stable/regression.html).\n",
    "Specific packages exist for the analysis of fMRI data, such as [linear](nilearn.github.io/) that we use below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Whole brain analysis of the \"localizer\" with encoding models  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation and contrast: Testing effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to analyze all voxels, not just an example.  \n",
    "**Q2-1: How does one call this type of regression which is repeated for every voxel?**\n",
    "\n",
    "We are going to use the same contrasts introduced above (plotted below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the contrasts\n",
    "for k, contrast_id in enumerate(['left - right click', 'audio - video']):\n",
    "    plt.subplot(2,1,k+1)\n",
    "    plt.bar(np.arange(len(contrasts[contrast_id])),\n",
    "            contrasts[contrast_id])\n",
    "    plt.plot([0, len(contrasts[contrast_id])-1], [0, 0], 'k')\n",
    "    plt.xlim(-0.5, len(contrasts[contrast_id])-0.5)\n",
    "    plt.title(contrast_id)\n",
    "plt.xticks(ticks=np.arange(len(contrasts['left - right click'])),\n",
    "          labels=[name for name in design_matrix.columns],\n",
    "          rotation=60, ha='right')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following line of code uses a function from nilearn, a Python package, to estimate the general linear model (GLM) on all voxels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate GLM on all voxels\n",
    "labels, estimates = run_glm(fMRI_data, design_matrix.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function run_glm does actually a bit more than the simple ordinary least square (OLS) solution used above with the formula. Check the help of the function (by running the next cell). It uses the option with ar1, which stands fro auto-regressive model.  \n",
    "**Q2-2 Why do we use an auto-regressive model for the estimation?**  \n",
    "Note that the OLS can also be used (but it is not the default option of run_glm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_glm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrated the temporal autocorrelation of the signal, we can plot the autocorrelation function for an example voxel.  \n",
    "**Q2-3 What is the autocorrelation in this voxel (correlation between successive values)?**  \n",
    "NB: What is problematic for regression model is actually the autocorrelation of the residual timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute and plot the autocorrelation function of an example voxel\n",
    "TR_range = range(1, 20)\n",
    "auto_correlation = [np.corrcoef(example_voxel[:-lag], example_voxel[lag:])[0, 1]\n",
    "                    for lag in TR_range]\n",
    "plt.plot(TR_range, auto_correlation, '-o')\n",
    "plt.ylabel('correlation')\n",
    "plt.xlabel('lag (# scans)')\n",
    "plt.grid(visible=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell computes the t-value of the contrast. This is similar to the computation done by hand above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate contrast\n",
    "contrast_id = 'left - right click'\n",
    "contrast = compute_contrast(labels, estimates,\n",
    "                            contrasts[contrast_id],\n",
    "                            contrast_type='t')\n",
    "t_val = masker.inverse_transform(contrast.stat())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to display the thresholded T-map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot glass brain\n",
    "plotting.plot_glass_brain(t_val, threshold=3, plot_abs=False,\n",
    "                          colorbar=True, title=contrast_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2-4 What is a glass brain representation?**  \n",
    "The following cell renders the results on slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot slices\n",
    "plotting.plot_stat_map(t_val, display_mode='z',\n",
    "                       threshold=3.0, title=contrast_id,\n",
    "                       cut_coords=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2-5: What is the orientation of the slices presented above?**  \n",
    "\n",
    "The following cell renders the results on the surface of the right hemisphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get canonical surface anatomy\n",
    "fsaverage = datasets.fetch_surf_fsaverage()\n",
    "\n",
    "# project the results onto the mesh of cortical surface\n",
    "texture = surface.vol_to_surf(t_val, fsaverage.pial_right)\n",
    "\n",
    "# plot\n",
    "plotting.plot_surf_stat_map(\n",
    "        fsaverage.infl_right, texture, hemi='right',\n",
    "        title=contrast_id, colorbar=True,\n",
    "        threshold=3.0, bg_map=fsaverage.sulc_right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2-6: What do the blue-ish and red-ish blobs correspond to?**  \n",
    "**Q2-7: Interpret the results.**  \n",
    "\n",
    "The next cell now estimates and reports the \"audio-visual\" contrast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate contrast\n",
    "contrast_id = 'audio - video'\n",
    "contrast_AV = compute_contrast(labels, estimates,\n",
    "                            contrasts[contrast_id],\n",
    "                            contrast_type='t')\n",
    "t_val_AV = masker.inverse_transform(contrast_AV.stat())\n",
    "plotting.plot_glass_brain(t_val_AV, threshold=3, plot_abs=False,\n",
    "                          colorbar=True, title=contrast_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2-8: Interpret the results.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of different noise models\n",
    "The above estimation and inference assumed autocorrelated noise with an AR one model.  \n",
    "Let's compare the results with the OLS estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate GLM on all voxels\n",
    "labels_OLS, estimates_OLS = run_glm(fMRI_data, design_matrix.values, noise_model='ols')\n",
    "\n",
    "# Estimate contrast\n",
    "contrast_id = 'left - right click'\n",
    "contrast_OLS = compute_contrast(labels_OLS, estimates_OLS,\n",
    "                            contrasts[contrast_id],\n",
    "                            contrast_type='t')\n",
    "t_val_OLS = masker.inverse_transform(contrast_OLS.stat())\n",
    "\n",
    "# Plot results\n",
    "plotting.plot_glass_brain(t_val_OLS, threshold=3, plot_abs=False,\n",
    "                          colorbar=True, title=contrast_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(contrast.stat(), contrast_OLS.stat(), '.')\n",
    "plt.plot([-12, 15], [-12, 15], 'k')\n",
    "plt.xlabel('AR1 estimate')\n",
    "plt.ylabel('OLS estimate')\n",
    "plt.title(f\"T-values\")\n",
    "plt.grid('on')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.9 Significance with OLS estimates is biased. In which direction? Why?**   \n",
    "\n",
    "Let's know compare the beta estimates themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(contrast.effect_size(), contrast_OLS.effect_size(), '.')\n",
    "plt.plot([-100, 200], [-100, 200], 'k')\n",
    "plt.xlabel('AR1 estimate')\n",
    "plt.ylabel('OLS estimate')\n",
    "plt.title('Beta-values')\n",
    "plt.grid('on')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.10 Are OLS estimates of beta weights biased? Should we care about the noise model for group-level inference?**   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical significance and correction for multiple comparisons\n",
    "The figure above is displayed with a threshold of +/- 3.  \n",
    "**Q3-10: Change the code to set a more liberal and a more conservative threshold.**  \n",
    "**Q3-11: Do you think that some voxels are actually false positives? Why?**  \n",
    "\n",
    "We are going to use parametric statistics (i.e. the assumption that the t-values follow a known distribution) to assess significance levels as p-values.  \n",
    "The following cell shows the map thresholded at p<0.001 (two-sided test, controlling for both positive and negative effects).  \n",
    "Technical detail: the t-values are transformed into z-values because it is more convenient to do so with the nilearn package.  \n",
    "The z-value corresponding to p<0.001 (two-sided test) uncorrected is displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_vals = contrast_AV.z_score()\n",
    "z_map = masker.inverse_transform(z_vals)\n",
    "alpha_threshold = 0.001\n",
    "z_thd = norm.isf(alpha_threshold/2)\n",
    "print(f\"FPR threshold (p={alpha_threshold}, two-sided test): z={z_thd:0.2f}\")\n",
    "plotting.plot_glass_brain(z_map, threshold=z_thd, plot_abs=False,\n",
    "                          colorbar=True, title=contrast_id+' uncorr.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell computes a new threshold that controls for a given false discovery rate (FDR). This estimation is done with the Benjamini-Hochberg procedure. For details on this procedure, and more generally, an introduction to multiple comparisons correction, see this [review by T. Nichols and S. Hayasaka, 2003](https://journals.sagepub.com/doi/pdf/10.1191/0962280203sm341ra).   \n",
    "The threshold (z-value) that corrects for the false discovery rate at p<0.001 (two-sided test) is reported and used to threshold the map.  \n",
    "**Q2-11: Interpret the difference between corrected and uncorrected maps**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdr_z_thd = fdr_threshold(np.abs(z_vals), alpha_threshold/2)\n",
    "print(f\"FDR threshold (p={alpha_threshold}, two-sided test): z={fdr_z_thd:0.2f}\")\n",
    "plotting.plot_glass_brain(z_map, threshold=fdr_z_thd, plot_abs=False,\n",
    "                          colorbar=True, title=contrast_id+' corr.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FDR is a weak correction. See the mentioned-aboved review for more conservative corrections. \n",
    "The family-wise error rate (FWER) correction is more conservative. When samples are assumed to be independent, the FWER correction is known as the Bonferroni correction. This correction is overly conservative in fMRI because the signals are spatially smooth (thus, not independent).\n",
    "Here is the result of the Bonferroni correction of our contrast of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_threshold = 0.05\n",
    "fwe_z_thd = norm.isf((alpha_threshold/2)/z_vals.shape[0])\n",
    "print(f\"FWE (Bonferroni) threshold (p={alpha_threshold}, two-sided test): z={fwe_z_thd:0.2f}\")\n",
    "plotting.plot_glass_brain(z_map, threshold=fwe_z_thd, plot_abs=False,\n",
    "                          colorbar=True, title=contrast_id+' corr.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In nilearn, you can use nilearn.glm.threshold_stats_img (with the option 'fpr' for uncorrected, 'fdr' and 'bonferroni') to perform those corrections, and exclude clusters smaller than a given threshold. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.glm import threshold_stats_img\n",
    "fwer_map_without_small_clusters, fwer_z_thd = threshold_stats_img(\n",
    "    z_map, alpha=alpha_threshold, height_control='bonferroni', cluster_threshold=50)\n",
    "print(f\"FWE (Bonferroni) threshold (p={alpha_threshold}, two-sided test): z={fwe_z_thd:0.2f}\")\n",
    "plotting.plot_glass_brain(fwer_map_without_small_clusters,\n",
    "                          threshold=fwer_z_thd, plot_abs=False,\n",
    "                          colorbar=True, title=contrast_id+' corr.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
